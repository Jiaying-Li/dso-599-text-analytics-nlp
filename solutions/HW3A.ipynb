{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 (Due 5:30pm PST April 2nd, 2019): N-Grams, Regex, and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit via Slack/email.\n",
    "\n",
    "You are an analyst working at McDonalds' corporate headquarters, and charged with identifying areas for improvement to increase customer service.\n",
    "\n",
    "Using the `mcdonalds-yelp-negative-reviews.csv` dataset, clean and parse the text reviews. Document the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "\n",
    "Finally, generate a TF-IDF report that **visualizes** for each city what the major source of complaints with the McDonalds franchises are. Offer your analysis and business recommendations on next steps for the global SVP of Operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief summary of my answers to the following four questions:\n",
    "- why remove/keep stopwords?\n",
    "I removed stopwords since in this way I can filter out all the frequent but meaningless words like 'the' and then find out the root cause of negative reviews.\n",
    "\n",
    "- stemming versus lemmatization?\n",
    "I used **lemmatization**, since it can keep the orginal meaning of words, which can be easier to understand compared with stemming.\n",
    "\n",
    "- regex cleaning and substitution?\n",
    "I applied a regex rule that requires words have more than 1 character to filter out less informative words. I used a **less strict** restriction compared with other regex rules that require words containing more characters, since valuable phrases like **'20 minutes'** will be filtered out under such circumstances. I do not want to miss out too much valuable insights in the data. \n",
    "\n",
    "- adding in custom stopwords?\n",
    "I added some custom stopwords. For example, I added words like **'mcdonalds' and 'always'** as they are not informative and cannot add valuable contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin1\")\n",
    "corpus = list(df[\"review\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "def lemmatize_sentence(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:            \n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(res_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lem=[]\n",
    "for review in corpus:\n",
    "    corpus_lem = corpus_lem + [lemmatize_sentence(review)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I decide to keep the stopwords and set several more words as stopwords. \n",
    "# The reason is that only in this way can I filter out all the frequent but meaningless words\n",
    "# and find out the root cause of negative reviews.\n",
    "# I added 'mcdonalds','mcdonald','macdonald','mcds', as all of which refers to Mcdonalds and cannot add valuable contents.\n",
    "# I also added words like 'ever','like','always', which cannot tell the area for improvement either.\n",
    "\n",
    "stop=stopwords.words('english')+['mcdonalds','mcdonald','macdonald','mcds','mc',\n",
    "                                 'ever','like','always','every',\n",
    "                                 'bad','worst','terrible','even','though']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I used regex to define the pattern of the token, which should contain more than 1 letters or numbers.\n",
    "# The reason is that I think words composed of only one letter or number actually cannot\n",
    "# contribute any meaningful information.\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,3),\n",
    "                             token_pattern=r'\\b[a-zA-Z0-9]{2,}\\b',\n",
    "                             max_df=0.5,\n",
    "                             min_df=1, stop_words=stop)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "score[\"term\"] = terms\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many cities there are in the dataset\n",
    "city = df.city.unique()\n",
    "city = list(city)\n",
    "city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_score(df,city):\n",
    "    df_city = df[df['city']==city]\n",
    "    corpus = list(df_city[\"review\"].values)\n",
    "    corpus_lem=[]\n",
    "    for review in corpus:\n",
    "        corpus_lem = corpus_lem + [lemmatize_sentence(review)]\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "    tf_idf = tf_idf.sum(axis=1)\n",
    "    score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "    score[\"term\"] = terms\n",
    "    score.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_atlanta = word_score(df, 'Atlanta')\n",
    "score_atlanta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lasvegas = word_score(df, 'Las Vegas')\n",
    "score_lasvegas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dallas = word_score(df, 'Dallas')\n",
    "score_dallas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_portland = word_score(df, 'Portland')\n",
    "score_portland.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_chicago = word_score(df, 'Chicago')\n",
    "score_chicago.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cleveland = word_score(df, 'Cleveland')\n",
    "score_cleveland.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_houston = word_score(df, 'Houston')\n",
    "score_houston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_la = word_score(df, 'Los Angeles')\n",
    "score_la.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ny = word_score(df, 'New York')\n",
    "score_ny.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a closer look at the reviews containing the key words\n",
    "\n",
    "## Note this is a good idea to do and what I considered an example of deep dive - a lot of us wrote that `drive thru` was a top issue, but it's important to actually dig into the reviews themselves to see what the context is - is it actually the drivethrough itself, is it the poor customer service, bad taste of products, lack of drive through, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = [string for string in corpus_lem if  \"drive thru\" in string]\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = [string for string in corpus_lem if  \"customer service\" in string]\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = [string for string in corpus if  \"ice cream\" in string]\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = [string for string in corpus if  \"10 minutes\" in string]\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = [string for string in corpus if  \"fast food\" in string]\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = [string for string in corpus_lem if  \"parking lot\" in string]\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = [string for string in corpus if  'roaches' in string]\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = [string for string in corpus if  'egg mcmuffin' in string]\n",
    "match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Complaints Analysis\n",
    "After analyzing 1525 negative customer reviews regarding McDonalds, I summarized mainly the following five issues, all of which are shown in the pictures.\n",
    "\n",
    "**- 1/Drive thru**\n",
    "\n",
    "Drive thru is one of the greatest sources of complaints. Customers are unsatisfied about waiting for a long time at drive thru and getting their orders wrong food.\n",
    "\n",
    "**- 2/Fast food**\n",
    "\n",
    "This appears frequently because customers are expecting fast service thus they are unsatisfied with the long waiting time to place orders and get their food.\n",
    "\n",
    "**- 3/Customer service**\n",
    "\n",
    "A lot of customers complaint that the staff at McDonald's had a bad attitude and did not show respect to customers.\n",
    "\n",
    "**- 4/Ice cream**\n",
    "\n",
    "Customers complaint that they cannot get ice cream since the machine was shut down at midnight.\n",
    "\n",
    "**- 5/Wrong order**\n",
    "\n",
    "Customers are angry about not getting the right food for their orders.\n",
    "\n",
    "\n",
    "Next, I would like to point out the sources of complaints in each city.\n",
    "\n",
    "Atlanta: the 1st, 2nd and 4th issues discussed above.\n",
    "\n",
    "Chicago: except for the 1st and 3rd issues, people are complaining about parking lots.\n",
    "\n",
    "Cleveland: the 1st, 2nd, 3rd issues.\n",
    "\n",
    "Dallas: the 1st, 2nd, 3rd issues. People are also complaining about parking lots.\n",
    "\n",
    "Houston: the 1st, 2nd, 3rd issues.\n",
    "\n",
    "Los Angeles: the 1st, 2nd, 5th issues.\n",
    "\n",
    "Las Vegas: the 1st, 2nd, 3rd, 5th issues.\n",
    "\n",
    "New York: except for the 1st and 2nd issues mentioned above, customers complaint about the sanitary condition, particularly the roaches.\n",
    "\n",
    "Portland: people are mainly complaining about the 1st, 2nd and 5th issues mentioned above. \n",
    "\n",
    "Customers order egg mcmuffin a lot but often get wrong food for their orders.\n",
    "\n",
    "Note: in the following pictures, the color and the size represent the frequency of the complaint sources. The darker the color and the bigger the box, the more frequent the words appear in customer negative reviews.\n",
    "<img src=\"image/All Cities.png\" style=\"width:600px;\"/>\n",
    "For the detailed pictures for each city, see appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation\n",
    "\n",
    "**1. Promote McDonald’s mobile ordering feature.** By encouraging customers to place orders on the mobile application, on the one hand, it can **improve working efficiency** for staff at McDonald’s and **avoid serving wrong food**; on the other hand, staff can have more time to prepare the food and **save the waiting time for customers**. \n",
    "\n",
    "**2. Design a new window or a drive-thru lane dedicated for mobile ordering pick-up.** Combining the first recommendation with this one, McDonald’s can **streamline the service** for customers who order online and save time for them.\n",
    "\n",
    "**3. Use staff training and development to motivate them.** Encourage staff to communicate with and serve customers more patiently and friendly. McDonald’s can **conduct employee evaluation and reward staff with good performance**.\n",
    "\n",
    "**4. Encourage customers to order using McDonald’s self-ordering kiosks.** In this way McDonald’s can avoid unpleasant customers experience with the staff and also ensure that customers can get the right food.\n",
    "\n",
    "**5. For McDonald’s with huge ice cream demand, consider adding another ice cream machine.** Ice cream machines cannot work 24/7 since they need time to be cleaned up. Taking into consideration that customers' demand and current supply, for McDonald’s where customers have huge demand for ice cream, we should consider adding another ice cream machine to meet the demand. On the one hand, we can ensure we are **not missing more opportunities to sell more products**; on the other hand, we can **improve our brand image and increase customer loyalty**. By ensuring ample supply, customers would also be encouraged to purchase more other food."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## NOTE: One thing this student could have done better is format the visualizations so that the text is a little bit larger (or at least proportional to the size of the relevance / tf-idf score)\n",
    "\n",
    "<img src=\"image/Atlanta.png\" style=\"width:600px;\"/>\n",
    "<img src=\"image/Las Vegas.png\" style=\"width:600px;\"/>\n",
    "<img src=\"image/Dallas.png\" style=\"width:600px;\"/>\n",
    "<img src=\"image/Portland.png\" style=\"width:600px;\"/>\n",
    "<img src=\"image/Chicago.png\" style=\"width:600px;\"/>\n",
    "<img src=\"image/Cleveland.png\" style=\"width:600px;\"/>\n",
    "<img src=\"image/Houston.png\" style=\"width:600px;\"/>\n",
    "<img src=\"image/la.png\" style=\"width:600px;\"/>\n",
    "<img src=\"image/ny.png\" style=\"width:600px;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
